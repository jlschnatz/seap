% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/loss.R
\name{compute_loss}
\alias{compute_loss}
\title{Compute the Kullback-Leibler divergence between the empirical and theoretical effect size sample size joint distribution}
\usage{
compute_loss(
  p,
  q,
  n_grid = 100,
  bounds = c(0.005, 0.995),
  start_nb = c(10, 100),
  start_norm = c(0, 0.5)
)
}
\arguments{
\item{p}{data frame or matrix of the empirical distribution with colnames n and d}

\item{q}{data frame or matrix of the theoretical distribution with colnames n and d}

\item{n_grid}{integer with the number of grid points (k^2) for the 2-dimensional kernel density estimation process}

\item{bounds}{vector of length 2 with lower and upper quantiles (default: (0.005, 0.995))}

\item{start_nb}{vector of length 2 with starting values for ML-estimation for phi and mu of negative binomial distribution (default: (10, 100)}

\item{start_norm}{vector of length 2 with starting values for ML-estimation for mu and sigma of normal distribution (default: (0, 0.5)}
}
\value{
numeric value of the Kullback-Leibler divergence
}
\description{
This function computes the Kullback-Leibler divergence between the empirical and theoretical effect size sample size joint distribution.
Empirical and theoretical distributions are estimated using 2-dimensional kernel density estimation with a grid size set by \code{n_grid}.
The lower and upper bounds for the grid are computed based on the absolute minimum and maximum of the data and the quantiles of the fitted distributions.
The starting values for the maximum likelihood estimation of the normal and negative binomial distributions can be set with \code{start_norm} and \code{start_nb}.
}
